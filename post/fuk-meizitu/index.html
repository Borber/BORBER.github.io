<html>
  <head>
    <meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>BORBER</title>
<meta name="description" content="世俗为枷锁 世人皆自缚" />
<link rel="shortcut icon" href="https://borber.github.io/favicon.ico?v=1572182072163">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.0/katex.min.css">
<link href="https://cdn.remixicon.com/releases/v1.3.1/remixicon.css" rel="stylesheet">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Droid+Serif:400,700">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.2/animate.min.css">

<link rel="stylesheet" href="https://borber.github.io/styles/main.css">
<link rel="alternate" type="application/atom+xml" title="BORBER - Atom Feed" href="https://borber.github.io/atom.xml">


<script async src="https://www.googletagmanager.com/gtag/js?id=UA-142130514-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-142130514-1');
</script>


  </head>
  <body>
    <div id="app" class="main px-4 flex flex-col lg:flex-row">
      <div id="sidebar" class="sidebar-wrapper lg:static lg:w-1/4">
  <div class="lg:sticky top-0">
    <div class="sidebar-content">
      <div class="flex lg:block p-4 lg:px-0 items-center fixed lg:static lg:block top-0 right-0 left-0 bg-white z-50">
        <i class="remixicon-menu-2-line lg:mt-4 text-2xl cursor-pointer animated fadeIn" onclick="openMenu()"></i>
        <a href="https://borber.github.io">
          <img class="animated fadeInLeft avatar rounded-lg mx-4 lg:mt-32 lg:mx-0 mt-0 lg:w-24 lg:h-24 w-12 w-12" src="https://borber.github.io/images/avatar.png?v=1572182072163" alt="">
        </a>
        <h1 class="animated fadeInLeft lg:text-4xl font-extrabold lg:mt-8 mt-0 text-xl" style="animation-delay: 0.2s">BORBER</h1>
      </div>
      
        <div class="animated fadeInLeft" style="animation-delay: 0.4s">
          <p class="my-4 text-gray-600 font-light hidden lg:block">
            文章目录
          </p>
          <div class="toc-container hidden lg:block">
            
          </div>
        </div>
      
    </div>
  </div>
</div>

<div class="menu-container">
  <i class="remixicon-arrow-left-line text-2xl cursor-pointer animated fadeIn close-menu-btn" onclick="closeMenu()"></i>
  <div>
    
      
        <a href="/" class="menu" style="animation-delay: 0s">
          首页
        </a>
      
    
      
        <a href="/archives" class="menu" style="animation-delay: 0.2s">
          归档
        </a>
      
    
      
        <a href="/tags" class="menu" style="animation-delay: 0.4s">
          标签
        </a>
      
    
      
        <a href="https://borber.github.io/toolkits" class="menu" style="animation-delay: 0.6000000000000001s">
          工具
        </a>
      
    
      
        <a href="https://borber.github.io/friend" class="menu" style="animation-delay: 0.8s">
          友链
        </a>
      
    
      
        <a href="/post/about" class="menu" style="animation-delay: 1s">
          关于
        </a>
      
    
  </div>
  <div class="site-footer">
    <div class="py-4 text-gray-700"><p>Powered by <a href="https://github.com/getgridea/gridea" target="_blank">Gridea</a> with <a href="https://github.com/Borber/gridea-theme-nederburgNewer">nederburgNewer</a> <small>&copy <a href="https://github.com/Borber" target="_blank">BORBER</a> </small></p>
</div>
    <a class="rss" href="https://borber.github.io/atom.xml" target="_blank">RSS</a>
  </div>
</div>
<div class="mask" onclick="closeMenu()">
</div>
      <div class="content-wrapper py-32 lg:p-8 lg:w-3/4 post-detail animated fadeIn">
        <h1 class="text-3xl font-bold lg:mt-16">Fu*k meizitu</h1>
        <div class="text-sm text-gray-700 lg:my-8">
          2019-07-29 / 7 min read
        </div>
        
          <img class="post-feature-image rounded-lg mx-auto my-4" src="https://www.z4a.net/images/2019/07/29/fall_by_wlop-d96fxqh.jpg" alt="">
        
        <div class="post-content yue">
          <p>爬取妹子图全站图片</p>
<!-- more -->
<p>由于我没有提前记录过程 所以没有截图，以后尽量把自己的思考过程都全部截图发出来。</p>
<p>首先通过 <a href="https://www.mzitu.com">首页</a> 的的所有分页面爬取所有的图片的首页链接 （写完之后发现有另一个界面 更容易爬 但是已经写完了就没有改了 这里放上链接 <a href="https://www.mzitu.com/all/">全部</a>）</p>
<p>这一步是很简单的， 主要就是吧真正的套图的链接全部整理出来，放在 mongo/meizitu/home 这个表里面，准备二次加工。</p>
<pre><code class="language-python">#home_url.py

from toolkits.ip_proxies import get_proxies
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import requests
import pymongo
import time

client = pymongo.MongoClient('localhost', 27017)
meizitu = client['meizitu']
home = meizitu['home']

ua = UserAgent()

headers = {
    'User-Agent': ua.random,
    'Referer': 'https://www.mzitu.com'
}


def get_home_url(i=0):
    url = f'https://www.mzitu.com/page/{i}'
    wb_data = requests.get(url, proxies=get_proxies(), headers=headers, timeout=3)
    soup = BeautifulSoup(wb_data.text, 'lxml')
    real = soup.select('#pins &gt; li &gt; span &gt; a')
    for index, each in enumerate(real):
        home.insert_one({'URL': each.get('href')})

</code></pre>
<p>接下来就是对爬去的链接做二次加工。</p>
<p>把每个链接对应的套图的 标题，链接，最大页码数 提取出来，因为妹子图的网页结构非常简单，每张图对应的URL 都是首页后面加页码，所以可以利用这个，减少储存的URL数。保存的 title 是作为以后的储存目录子文件夹名。</p>
<p>以及一个用于之后下载时提取每个界面图片的函数（get_imgs(url)）</p>
<p>这里尤其要说 关于</p>
<pre><code class="language-python">for i in range(10):
</code></pre>
<p>的作用。</p>
<p>不知道是由于网站的反扒措施，还是我使用的代理ip池中有ip失效，总之会有request中断的现象，总会导致爬虫强制退出，这一退出就是浪费几十分钟的时间，所以加上 try 的报错处理之后，程序的鲁棒性会提高非常多，这个结构也出现在随后的几个py文件中，随后不在作解释。</p>
<p>这里也强烈建议使用 IP代理 可以参考我的上一篇博文。都打包好了可以直接使用。</p>
<pre><code class="language-python">#find_imgs.py

from toolkits.ip_proxies import get_proxies
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import requests
import pymongo
import time


client = pymongo.MongoClient('localhost', 27017)
meizitu = client['meizitu']
home = meizitu['home']
imgs = meizitu['imgs']

ua = UserAgent()


def get_title_and_pages(url):
    headers = {
        'User-Agent': ua.random,
        'Referer': 'https://www.mzitu.com'
    }
    for i in range(10):
        try:
            wb_data = requests.get(url, headers=headers, proxies=get_proxies())
            while wb_data != 200:
                wb_data = requests.get(url, headers=headers,proxies=get_proxies())
        except Exception:
            if i &gt;= 9:
                print('Fuck!')
            else:
                time.sleep(0.5)
        else:
            break

    title = url[-6:]
    soup = BeautifulSoup(wb_data.text, 'lxml')
    titles = soup.select('div.main h2')
    for each in titles:
        title = each.get_text()
        break
    url_max = soup.select('div.pagenavi &gt; a &gt; span')[-2].get_text()
    imgs.insert_one({'title': title, 'url': url, 'url_max': int(url_max)})


def get_imgs(url):
    headers = {
        'User-Agent': ua.random,
        'Referer': 'https://www.mzitu.com'
    }
    for i in range(10):
        try:
            wb_data = requests.get(url, headers=headers, proxies=get_proxies())
            while wb_data.status_code != 200:
                wb_data = requests.get(url, headers=headers, proxies=get_proxies())
        except Exception:
            if i &gt;= 9:
                print('Fuck!')
            else:
                time.sleep(0.5)
        else:
            break
    soup = BeautifulSoup(wb_data.text, 'lxml')
    img_url = soup.select('div.main-image img')[0].get('src')
    return img_url



</code></pre>
<p>接下来是下载模块</p>
<pre><code class="language-python">#download_imgs.py

from toolkits.ip_proxies import get_proxies
from fake_useragent import UserAgent
import requests
import os
import time

seq = '/'
ua = UserAgent()

headers = {
    'User-Agent': ua.random,
    'Referer': 'https://www.mzitu.com'
}


def downimg(index, url, path):
    print(url)
    filename = path + seq[0] + str(index) + url[-4:]
    print(filename)
    if os.path.exists(filename):
        return
    for i in range(10):
        try:
            r = requests.get(url, headers=headers, proxies=get_proxies(), stream=True)
        except Exception :
            if i &gt;= 9:
                print('Fuck!')
            else:
                time.sleep(0.5)
        else:
            time.sleep(0.1)
            break
    with open(filename, 'wb') as code:
        for img_data in r.iter_content(128):
            code.write(img_data)


</code></pre>
<p>最后是主函数：</p>
<p>这里面有一个大坑，坑的无与伦比，这也是这个爬虫我学到的比较重要的东西之一，pymongo创造的链接，是有时效性的，及时加了 no_cursor_timeout=True 他本身的链接就是以后时效性的，所以，出了跑错处理，还需要进行重连。</p>
<pre><code class="language-python">def download_all():
    client_local = pymongo.MongoClient('localhost', 27017)
    meizitu_local = client_local['meizitu']
    imgs_local = meizitu_local['imgs']
    items = imgs_local.find(no_cursor_timeout=True)
</code></pre>
<p>这一段 就是在每一次进行 下载 调用时都进行一次重连。当然这一般就是在发生链接断开错误时才会再次调用的。（据网上资料，链接有效时间为10分钟）</p>
<pre><code class="language-python">#main.py

from test.home_url import get_home_url
from test.find_imgs import get_title_and_pages,get_imgs
from test.download_img import downimg
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
import requests
import pymongo
import os
import time

client = pymongo.MongoClient('localhost', 27017)
meizitu = client['meizitu']
home = meizitu['home']
imgs = meizitu['imgs']

rootpath = '/home/x/BORBER/File/Tmp/meizitu' # 自己改
seq = '/'


def mkdir(path):
    folder = os.path.exists(path)
    if not folder:
        os.makedirs(path)


def home_all_url():
    for i in range(1, 226):
        get_home_url(i)
        print(f'Now in {i}')


def imgs_all_url():
    for index, each in enumerate(home.find(no_cursor_timeout=True)):
        print(f'Now in {index}')
        print(each['URL'])
        get_title_and_pages(each['URL'])
        home.delete_one({'URL': each['URL']})


def download_all():
    client_local = pymongo.MongoClient('localhost', 27017)
    meizitu_local = client_local['meizitu']
    imgs_local = meizitu_local['imgs']
    items = imgs_local.find(no_cursor_timeout=True)
    for item in items:
        filepath = rootpath + seq[0] + item['title']
        print(filepath)
        mkdir(filepath)
        for i in range(1, item['url_max']):
            downimg(i, get_imgs(item['url']+seq[0]+str(i)), filepath)
        imgs.delete_one({'url': item['url']})
    items.close()


def safe_download():
    for i in range(9999):
        try:
            download_all()
        except Exception:
            print('Fuck')
            time.sleep(0.5)
            download_all()
        else:
            break


def clear_all():
    meizitu.drop_collection('home')
    meizitu.drop_collection('imgs')


if __name__ == '__main__':
    home_all_url()
    imgs_all_url()
    safe_download()

</code></pre>
<p>此版本理论上可以一次运行成功 爬取全站 17万+图片 共计18.1G数据</p>
<p><img src="https://www.z4a.net/images/2019/07/29/16286196bfda2cc08.png" alt="16286196bfda2cc08.png"><br>
<img src="https://www.z4a.net/images/2019/07/29/2c6341d3c5e38c041.png" alt="2c6341d3c5e38c041.png"><br>
<img src="https://www.z4a.net/images/2019/07/29/3a94d521745bd3db4.png" alt="3a94d521745bd3db4.png"></p>
<p>身体是革命的本钱啊 骚年， 滑稽</p>

        </div>

        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://borber.github.io/tag/zzy4LPH8S">
            <span class="flex-auto">python</span>
          </a>
        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://borber.github.io/tag/C-16BRfGV">
            <span class="flex-auto">爬虫</span>
          </a>
        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://borber.github.io/tag/mR79ZebU2">
            <span class="flex-auto">编程</span>
          </a>
        
          <a class="animated fadeInUp p-2 items-center text-sm text-gray-700 border hover:bg-gray-300 leading-none rounded-full flex lg:inline-flex m-4 " href="https://borber.github.io/tag/qlm952wiz">
            <span class="flex-auto">NSFW</span>
          </a>
        


        <div class="flex justify-between py-8">
          
            <div class="prev-post">
              <a href="https://borber.github.io/post/python-toolkits">
                <h3 class="post-title">
                  <i class="remixicon-arrow-left-line"></i>
                  Python toolkits
                </h3>
              </a>
            </div>
          

          
            <div class="next-post">
              <a href="https://borber.github.io/post/ips-pool">
                <h3 class="post-title">
                  IP&#39;s pool
                  <i class="remixicon-arrow-right-line"></i>
                </h3>
              </a>
            </div>
          
        </div>

        
          
            <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
<script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>

<div id="gitalk-container"></div>

<script>

  var gitalk = new Gitalk({
    clientID: '46faa605211a1eadeb3a',
    clientSecret: '78d262d72d068322632d74ff2a80439a5c08ead1',
    repo: 'BORBER.github.io',
    owner: 'Borber',
    admin: ['Borber'],
    id: (location.pathname).substring(0, 49),      // Ensure uniqueness and length less than 50
    distractionFreeMode: false  // Facebook-like distraction free mode
  })

  gitalk.render('gitalk-container')

</script>

          

          
        

      </div>
    </div>

    <script src="https://borber.github.io/media/prism.js"></script>  
<script>

Prism.highlightAll()

let mainNavLinks = document.querySelectorAll(".markdownIt-TOC a");

// This should probably be throttled.
// Especially because it triggers during smooth scrolling.
// https://lodash.com/docs/4.17.10#throttle
// You could do like...
// window.addEventListener("scroll", () => {
//    _.throttle(doThatStuff, 100);
// });
// Only not doing it here to keep this Pen dependency-free.

window.addEventListener("scroll", event => {
  let fromTop = window.scrollY;

  mainNavLinks.forEach((link, index) => {
    let section = document.getElementById(decodeURI(link.hash).substring(1));
    let nextSection = null
    if (mainNavLinks[index + 1]) {
      nextSection = document.getElementById(decodeURI(mainNavLinks[index + 1].hash).substring(1));
    }
    if (section.offsetTop <= fromTop) {
      if (nextSection) {
        if (nextSection.offsetTop > fromTop) {
          link.classList.add("current");
        } else {
          link.classList.remove("current");    
        }
      } else {
        link.classList.add("current");
      }
    } else {
      link.classList.remove("current");
    }
  });
});


document.addEventListener("DOMContentLoaded", function() {
  var lazyImages = [].slice.call(document.querySelectorAll(".post-feature-image.lazy"));

  if ("IntersectionObserver" in window) {
    let lazyImageObserver = new IntersectionObserver(function(entries, observer) {
      entries.forEach(function(entry) {
        if (entry.isIntersecting) {
          let lazyImage = entry.target
          lazyImage.style.backgroundImage = `url(${lazyImage.dataset.bg})`
          lazyImage.classList.remove("lazy")
          lazyImageObserver.unobserve(lazyImage)
        }
      });
    });

    lazyImages.forEach(function(lazyImage) {
      lazyImageObserver.observe(lazyImage)
    })
  } else {
    // Possibly fall back to a more compatible method here
  }
});

const menuContainer = document.querySelector('.menu-container')
const menus = document.querySelectorAll('.menu-container .menu')
const mask = document.querySelector('.mask')
const contentWrapper = document.querySelector('.content-wrapper')
const latestArticle = document.querySelector('.latest-article')
const readMore = document.querySelector('.read-more')
const indexPage = document.querySelector('.index-page')

const isHome = location.pathname === '/'
if (latestArticle) {
  latestArticle.style.display = isHome ? 'block' : 'none'
  readMore.style.display = isHome ? 'block' : 'none'
  indexPage.style.display = isHome ? 'none' : 'block'
}

const openMenu = () => {
  menuContainer.classList.add('open')
  menus.forEach(menu => {
    menu.classList.add('animated', 'fadeInLeft')
  })
  mask.classList.add('open')
  contentWrapper.classList.add('is-second')
}

const closeMenu = () => {
  menuContainer.classList.remove('open')
  menus.forEach(menu => {
    menu.classList.remove('animated', 'fadeInLeft')
  })
  mask.classList.remove('open')
  contentWrapper.classList.remove('is-second')
}
</script>
  
  </body>
</html>
