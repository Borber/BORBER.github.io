---
title: 'Second python spider'
date: 2019-07-16 16:30:35
tags: [python,çˆ¬è™«,NSFW,ç¼–ç¨‹,æ¼«ç”»]
published: true
hideInList: false
feature: https://www.z4a.net/images/2019/07/16/6b8c14f8gw1f0vg7ga9raj21040lzdmx.png
---
 çˆ¬å–[zeroæ¬è¿ç½‘](http://www.zerobyw4.com/) æ¼«ç”» æœ‰ç¦åˆ©å“¦ ğŸ¤ª
 
<!-- more -->

 æœ‹å‹( [Miracoi](https://miracol.cn/) )è¯´è‡ªå·±çš„æ³¡é¢æ¿æ²¡æ¼«ç”»å¯çœ‹,æˆ‘ä¹Ÿæ­£å¥½æ˜¨å¤©å¼€å§‹å­¦çˆ¬è™« çœ‹çœ‹èƒ½ä¸èƒ½çˆ¬äº›æ¼«ç”»ç»™ä»–çœ‹,æˆ‘å°±æ‰¾äº†ä¸€ä¸ªæ¼«ç”»ç½‘ç«™,å¼€å§‹äº†.

[zeroæ¬è¿ç½‘](http://www.zerobyw4.com/) çœ‹äº†ä¸€çœ¼ æ³¨å†Œç”¨æˆ·å¯ä»¥çœ‹ é™¤æœ€æ–°ç« èŠ‚çš„æ‰€æœ‰,ä½†æ˜¯å¯¹åº”çš„ä¸‹è½½æœ‰æ¬¡æ•°é™åˆ¶,å¯è§‚çœ‹æ²¡æœ‰å•Š.å˜¿å˜¿å˜¿ æèµ·.

1. æ³¨å†Œ ç™»å½•

2. æ‹¿åˆ° cookie å…·ä½“å¯ä»¥çœ‹ æˆ‘å‘å¸ƒçš„ [First python reptile](https://borber.xyz/post/first-python-reptile/) å…¶ä¸­ç¬¬ä¸‰æ­¥

3. æ›¿æ¢ä¸‹é¢ä»£ç çš„ cookie ä»¥åŠ ä½ çš„æ–‡æ¡£å‚¨å­˜åœ°å€

   ```python
   from bs4 import BeautifulSoup
   import requests
   import os
   header = {
       'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3833.0 Safari/537.36',
       'Cookie':'ä½ è‡ªå·±çš„'
   
   }
   web_head = 'http://www.zerobyw4.com'
   rootpath = 'E:\File\Tmp\ ' #è‡ªå·±æ”¹
   seq = '\ '
   filend = '.jpg'
   
   def mkdir(path):
       folder = os.path.exists(path)
   
       if not folder:  # åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ–‡ä»¶å¤¹å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºä¸ºæ–‡ä»¶å¤¹
           os.makedirs(path)  # makedirs åˆ›å»ºæ–‡ä»¶æ—¶å¦‚æœè·¯å¾„ä¸å­˜åœ¨ä¼šåˆ›å»ºè¿™ä¸ªè·¯å¾„
           print
           "---  new folder...  ---"
           print
           "---  OK  ---"
   
       else:
           print
           "---  There is this folder!  ---"
   
   
   
   def start():
       for NUM in range(1,46):
           url = 'http://www.zerobyw4.com/plugin.php?id=jameson_manhua&c=index&a=ku&&page=' +str(NUM)
           web_data = requests.get(url)
           Soup = BeautifulSoup(web_data.text, 'lxml')
           urls = Soup.select('div.uk-card > div > a')
           for each in urls:
               one(web_head+each.get('href')[1:])
   
   def one(url):
       web_data = requests.get(url)
       Soup = BeautifulSoup(web_data.text, 'lxml')
       details = Soup.select('a.uk-button-default')
       title = Soup.select('h3.uk-heading-line')
       folderpath = ''
       for each in title:
           if len(each.get_text()) > 4:
               folderpath = rootpath[:-1] + each.get_text()
               mkdir(folderpath)
               break
       chapterNum = 0
       for each in details:
           filepath = folderpath+seq[0]+str(chapterNum)
           mkdir(filepath)
           chapterNum = chapterNum+1
           realimg(web_head+each.get('href')[1:],filepath)
   
   
   def realimg(url,filepath):
       web_data = requests.get(url,headers=header)
       Soup = BeautifulSoup(web_data.text, 'lxml')
       imgs = Soup.select('div.mb0 > img')
       NUM = 0
       for img in imgs:
           downimg(img.get('src'),filepath,NUM)
           NUM = NUM+1
   
   
   def downimg(url,filepath,NUM):
       r = requests.get(url, stream=True)
       if r.status_code != 200:
           return
       filename = filepath+seq[0]+str(NUM)+filend
       with open(filename, 'wb') as code:
          for img_data in r.iter_content(128):
              code.write(img_data)
   
   start()
   
   ```

    æ¼«ç”»å“ªæœ‰ pythonæœ‰è¶£ å˜¿å˜¿å˜¿ ğŸ˜ƒ