---
title: 'Third python spider'
date: 2019-07-16 22:07:02
tags: [python,çˆ¬è™«,ç¼–ç¨‹]
published: true
hideInList: false
feature: https://www.z4a.net/images/2019/07/16/2018-07-13-010717.jpg
---
çˆ¬å– 58åŒåŸ æ‹›è˜ä¿¡æ¯ ğŸ¤©

<!-- more -->

è¿™æ˜¯æˆ‘å†™çš„çš„ç¬¬ä¸‰ä¸ªçˆ¬è™«ç”¨æ¥çˆ¬å– 58åŒåŸä¸Šçš„æ‹›è˜ä¿¡æ¯ ä¹Ÿæ²¡æœ‰ä»€ä¹ˆå¤§ç”¨åªæ˜¯ç”¨æ¥ç»ƒæ‰‹çš„,æˆ‘è§‰å¾— ç¼–ç¨‹æ˜¯ä¸€ä¸ªåªæœ‰åŠ¨æ‰‹æ‰èƒ½å­¦ä¼šçš„ä¸œè¥¿.

è¿™æ¬¡çš„çˆ¬è™«å’Œå‰ä¸¤æ¬¡æœ‰äº›è®¸ä¸åŒ,58 åŒåŸæœ‰è‡ªå·±çš„åçˆ¬æªæ–½.æ‰€ä»¥è¿™æ¬¡åŠ ä¸Šäº†é™åˆ¶è¯·æ±‚é¢‘ç‡çš„ time.sleap (å…¶å® ç¬¬äºŒä¸ªçˆ¬è™«ä¹Ÿæœ€å¥½åŠ å…¥,å› ä¸ºæˆ‘åœ¨ä½¿ç”¨ç¬¬äºŒä¸ªçˆ¬è™«æ—¶,å‡ºç°äº†è¿œç¨‹æœåŠ¡å™¨æ‹’ç»è®¿é—®)

è¿™æ­¤å¯¹äº selector æœ‰çš„å…¨æ–°çš„è®¤è¯† ä½¿ç”¨å¤§å¤§çµæ´»äº†

58åŒåŸä¸éœ€è¦æ³¨å†Œå°±å¯ä»¥æµè§ˆ æ‰€ä»¥åªè¦è·å–cookieå°±å¥½

ä»£ç å¦‚ä¸‹ :

```python
from bs4 import BeautifulSoup
import os
import time
import urllib
import requests
import time

header = {
    'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3833.0 Safari/537.36',
    'cookie':'ä½ è‡ªå·±çš„'
}
urls = []
file = open('E:\CODE\python\out\BORBER.txt','w') #è‡ªå·±å†™
def get_urls():
    for i in range(1,19):
        url = f'https://bj.58.com/ruanjiangong/pn{i}/?PGTID=0d302f82-0000-1864-07d2-bb7c42c41f7c&ClickID=2'
        urls.append(url)
    for each in urls:
        get_detials(each)

def get_detials(url):
    wb_data = requests.get(url)
    soup = BeautifulSoup(wb_data.text,'lxml')
    Address = soup.select('span.address')
    Job_names = soup.select('span.name')
    Job_salarys = soup.select('p.job_salary')
    Job_wels = soup.select('div.job_wel > span')
    Com_Names = soup.select('div.comp_name > a')
    Job_requires = soup.select('p.job_require')
    for Addres,Job_name,Job_salary,Job_wel,Com_Name,Job_require in zip(Address,Job_names,Job_salarys,Job_wels,Com_Names,Job_requires):
        data = {
            'Address':Addres.get_text(),
            'Job_name':Job_name.get_text(),
            'Job_salary':Job_salary.get_text(),
            'Job_wel':Job_wel.get_text(),
            'Com_Name':Com_Name.get_text(),
            'Job_require':Job_require.get_text().replace('&nbsp',' ')
        }
        fileprint(data)
def fileprint(data):
    file.write('åœ°å€: ' + data['Address'])
    file.write('\n')
    file.write('èŒä½: '+data['Job_name'])
    file.write('\n')
    file.write('è¦æ±‚: '+data['Job_require'])
    file.write('\n')
    file.write('è–ªé…¬: '+data['Job_salary'])
    file.write('\n')
    file.write('ç¦åˆ©: '+data['Job_wel'])
    file.write('\n')
    file.write('å…¬å¸: '+data['Com_Name'])
    file.write('\n')
    file.write('\n===================================\n')

get_urls()
for url in urls:
    get_detials(url)
    time.sleep(2)
```

çœ‹äº†ä¸€ä¸‹ äº¬åŸå°±æ˜¯äº¬åŸå•Š ğŸ˜…



é™„:[çˆ¬å–ç»“æœ](https://www.lanzous.com/i50oi6d
) 